{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data science is OCEMN. Numpy - Python's linear algebra library - can help with obtaining, cleaning, and exploring data, leaving modeling and interpretation to other libraries. Genfromtxt is more flexible and powerful than loadtxt, so we'll explore its use in obtaining data. Preliminaries: conda install numpy, or if you're a bad person, pip install numpy. Import numpy as np. There are several - many - functions to create numpy arrays to play with and and many more to play with them, but for the purposes of data science our first step is to obtain data. Playground functions, therefore, are in the appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider Project Euler's tenth problem, summation of primes. In Project Euler itself it is easy to brute force it, but it is more nuanced in HackerRank, where there might be tens of thousands of test cases; speed and elegance are important.\n",
    "\n",
    "\n",
    "The sum of the primes below N:\n",
    "\n",
    "Find the sum of all the primes not greater than N. The first line contains the number of test cases T. The next T lines will contain Ns: https://www.hackerrank.com/contests/projecteuler/challenges/euler010/submissions/code/1311394025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vanilla Python\n",
    "\n",
    "#s=Sieve of Eratosthenes\n",
    "\n",
    "s = [False, True] * 500000\n",
    "\n",
    "#i=iterator\n",
    "\n",
    "for i in range (3,1000,2):\n",
    "    if s[i]:\n",
    "        for m in range (i**2,1000000,i*2):\n",
    "            s[m]=False\n",
    "\n",
    "#sop=list of sum of primes\n",
    "\n",
    "sop=[0,0,2]\n",
    "sum = 2\n",
    "for i in range(3,1000000):\n",
    "    if s[i]:\n",
    "        sum+=i\n",
    "    sop.append(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NumPython\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of `my_array`'s dimensions\n",
    "print(my_array.ndim)\n",
    "\n",
    "# Print the number of `my_array`'s elements\n",
    "print(my_array.size)\n",
    "\n",
    "# Print information about `my_array`'s memory layout\n",
    "print(my_array.flags)\n",
    "\n",
    "# Print the length of one array element in bytes\n",
    "print(my_array.itemsize)\n",
    "\n",
    "# Print the total consumed bytes by `my_array`'s elements\n",
    "print(my_array.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "py_arr = [1,2,3,4,5,6]\n",
    "numpy_arr = np.array([1,2,3,4,5,6])\n",
    "\n",
    "sizeof_py_arr = sys.getsizeof(1) * len(py_arr)           # Size = 168\n",
    "sizeof_numpy_arr = numpy_arr.itemsize * numpy_arr.size   # Size = 48\n",
    "\n",
    "py_arr = [1.1, 1.2,1.3424242, 1.436654646546, 454353.1232131, 32434353.324324, 1234.2321]\n",
    "numpy_arr = np.array([1.1, 1.2,1.3424242, 1.436654646546, 454353.1232131, 32434353.324324, 1234.2321])\n",
    "\n",
    "sizeof_py_arr = sys.getsizeof(12345.232343) * len(py_arr)           # Size  = 196\n",
    "sizeof_numpy_arr = numpy_arr.itemsize * numpy_arr.size   # Size = 56\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above examples, it turns out, are a whole string lies starting with the 8 byte cost of a 24 byte int is the pointer and the actual number and its memory cost is cleverly hidden elsewhere. In short, numpy is much smaller. Exactitude is a rabbit hole that sys.getsizeof() and its recursive big brother deep_getsizeof do not get to the bottom of; things like the list of small integers from -5 to 256 that python keeps as a matter of course; free, sort of. Fixed overhead, anyway. CPython manages small objects (less than 256 bytes) in special pools on 8-byte boundaries. There are pools for 1-8 bytes, 9-16 bytes... 249-256 bytes. When an object of size 9 is allocated, it is allocated from the 16-byte pool for objects 9-16 bytes in size. So, even though it contains only 9 bytes of data, it will cost 16 bytes of memory. If you allocate a million objects of size 9, you actually use 16 megabytes not 9. This overhead - 800% for 1 byte data - can hurt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "size = 1000\n",
    "\n",
    "def vanilla():\n",
    "    t1 = time.time()\n",
    "    X = range(size)\n",
    "    Y = range(size)\n",
    "    Z = [X[i] + Y[i] for i in range(len(X)) ]\n",
    "    return time.time() - t1\n",
    "\n",
    "def numpy_version():\n",
    "    t1 = time.time()\n",
    "    X = np.arange(size)\n",
    "    Y = np.arange(size)\n",
    "    Z = X + Y\n",
    "    return time.time() - t1\n",
    "\n",
    "\n",
    "t1 = vanilla()\n",
    "t2 = numpy_version()\n",
    "print(t1, t2)\n",
    "print(\"Numpy is in this example \" + str(t1/t2) + \" faster!\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
